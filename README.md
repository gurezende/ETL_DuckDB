# Data Pipeline with API Integration and AI Analysis
![](img/coverimage-wide.jpeg)

## Overview

This project demonstrates an ETL (Extract, Transform, Load) pipeline for a hypothetical company with chain of 5 stores, processing 1000 transactions a day. <br>
Being a small business, it should not have to go to expensive solutions like Databricks to process large quantities of data. So, we present DuckDB as a good solution.

**Here is the flow of the data in this pipeline:**

1. **FastAPI**: API integration, with data extracted daily (or customizable for the needed cadence)
2. **DuckDB** to quickly process, transform and load the clean data to a DB. The choice of DuckDB technology was due to its ability to process large sets of data locally and fastly.
3. Price Optimization with Generalized Additive Model (**PyGAM**). The clean data is promptly analyzed to find the optimized price for each of the products.
4. A Report is generated with the KPIs.
5. AI Agent: An AI Agent by **Agno** analyzes the content of the KPIs for the day and generates a report.
6. This report is automatically sent via email for a manager.

## Features

* Extracts data from a source (simulated in this case).
* Transforms the data using Python and DuckDB.
* Loads the transformed data into a DuckDB database.
* Calculate KPIs.
* Estimate optimal price with PyGAM.
* Analyzes data with AI Agent.
* Sends email automatically.


## Requirements

* Python 3.8+
* `duckdb` >= 1.2.2
* `fastapi` >= 0.115.12
* `pandas` >= 2.2.3
* `uvicorn` >= 0.34.2
* `numpy` >= 2.2.5
* `requests` >= 2.32.3
* `pyarrow` >= 20.0.0
* `fastparquet` >= 2024.11.0
* `agno` > =1.5.2
* `google-genai` >= 1.16.1
* `matplotlib` >= 3.10.3
* `tabulate` >= 0.9.0
* `pygam` >= 0.8.0
* `plotnine` >= 0.14.5

## Code Structure

* `extract.py`: This script is responsible for extracting data from a source. It likely simulates data extraction in this example.
* `transform.py`: This script transforms the extracted data. It may involve cleaning, filtering, and aggregating the data using DuckDB.
* `load.py`: This script loads the transformed data into a DuckDB database.
* `price_optimization`: Script to estimate the optimal price per product using a Generalize Additive Model.
* `report.py`: This script analyzes data and creates the report in markdown format. To be used as a tool by the AI Agent.
* `utils.py`: Script to automate email with the report.
* `main.py`: This is the main script to orchestrate the whole etl process, AI Agent Analysis and e-mail automation.

## How to Run Project

1. Clone the repository.
  
```bash
git clone https://github.com/gurezende/ETL_DuckDB
```

2. Create a virtual environment

```python
python -m venv .venv
```
  
3. Activate the virtual environment
  
```bash
source .venv/bin/activate (Linux/macOS) or .venv\Scripts\activate (Windows)
```

4. Install the dependencies
  
 ```python
pip install -r requirements.txt
 ```

5. Run the API that generates the data

```bash
uvicorn scripts.api:app --reload
```

6. Run the main script

```bash
python main.py
```

## KPIs Generated

**By Store:**<br>

* The total quantity sold of each product <br>

* The total revenue generated from each product by store <br>
* The total revenue generated by store <br>

**By Product:**<br>

* The total quantity sold of each product<br>
* The average price of each product<br>
* The average quantity sold of each product<br>
* The total revenue generated from each product<br>

## License

Project using MIT License

## Demonstration

<table>
  <tr>
    <td width="33%"><img src="img/E-mail_Report.png" alt="e-mail Report"></td>
    <td width="33%"><img src="img/price_optimization.png" alt="Price Optimization"></td>
    <td width="33%"><img src="img/API_DuckDB.gif" alt="App GIF"></td>
  </tr>
</table>

## Known Issues

There were some issues adding the project to Docker due to incompatibilities among the packages.

## About Me

This project was created by Gustavo R Santos (https://gustavorsantos.me)