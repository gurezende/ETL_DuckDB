# Data Pipeline with API Integration and AI Analysis

## Overview

This project demonstrates an ETL (Extract, Transform, Load) pipeline for a hypothetical company with chain of 5 stores, processing 1000 transactions a day. <br>
Being a small business, it should not have to go to expensive solutions like Databricks to process large quantities of data. So, we present DuckDB as a good solution.

**Here is the flow of the data in this pipeline:**

1. **FastAPI**: API integration, with data extracted daily (or customizable for the needed cadence)
2. **DuckDB** to quickly process, transform and load the clean data to a DB. The choice of DuckDB technology was due to its ability to process large sets of data locally and fastly.
3. Price Optimization with Generalized Additive Model (**PyGAM**). The clean data is promptly analyzed to find the optimized price for each of the products.
4. A Report is generated with the following KPIs.

By Store:<br>

* The total quantity sold of each product <br>
* The total revenue generated from each product by store <br>
* The total revenue generated by store <br>

By Product:<br>

* The total quantity sold of each product<br>
* The average price of each product<br>
* The average quantity sold of each product<br>
* The total revenue generated from each product<br>

5. AI Agent: An AI Agent by **Agno** analyzes the content of the KPIs for the day and generates a report.
6. This report is automatically sent via email for a manager.


- Features:
  - Extracts data from a source (simulated in this case).
  - Transforms the data using Python and DuckDB.
  - Loads the transformed data into a DuckDB database.

- Requirements:
  - Python 3.8+
  - DuckDB
  - pyproject.toml (includes dependencies: duckdb)

- Code Structure:
  - `extract.py`: This script is responsible for extracting data from a source. It likely simulates data extraction in this example.
  - `transform.py`: This script transforms the extracted data. It may involve cleaning, filtering, and aggregating the data using DuckDB.
  - `load.py`: This script loads the transformed data into a DuckDB database.
  - `main.py`: This is the main script to orchestrate the whole etl process.

- How to Run Project:
  1. Clone the repository.
  2. Create a virtual environment: `python -m venv .venv`
  3. Activate the virtual environment: `source .venv/bin/activate` (Linux/macOS) or `.venv\Scripts\activate` (Windows)
  4. Install the dependencies: `pip install -r requirements.txt` or `poetry install`
  5. Run the main script: `python main.py`

- License: MIT License

- Demonstration:
  (Will be added later)

- Known Issues:
  - There were some issues adding the project to Docker due to incompatibilities among the packages.

- About Me: This project was created by Gustavo R Santos (https://gustavorsantos.me)